{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"xxx\""
      ],
      "metadata": {
        "id": "rLBjm-E3tNEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Zw1b6YqW8jHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-hNx21t73nB",
        "outputId": "b2bd8209-ec69-49a1-e96d-243276346922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community tiktoken openai faiss-cpu langchain_openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY0WFnbptt-m",
        "outputId": "60e44390-a862-4a78-8889-ee4b28171483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1，管道操作符"
      ],
      "metadata": {
        "id": "HDa0T_SDtJvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dB4h-7_tEuM",
        "outputId": "f2650134-1cb6-443f-e067-35daef97145c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are five fruits:\n",
            "\n",
            "1. Apple\n",
            "2. Banana\n",
            "3. Orange\n",
            "4. Strawberry\n",
            "5. Mango\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"List 5 {topic}:\")\n",
        "model = OpenAI(model_name='gpt-4o-mini-2024-07-18')  #默认使用的是 gpt-3.5-turbo-instruct\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "chain = prompt | model #| output_parser\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fruits\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ph87QKXx86AL",
        "outputId": "cd1bf870-9a06-42ae-a5a5-4aec3debee2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gpt-4o-mini-2024-07-18'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrcBZJK6rv6c",
        "outputId": "7632c587-1222-4f2f-e3ae-cc17c56438fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['topic'], template='List 5 {topic}:')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2, 字典映射"
      ],
      "metadata": {
        "id": "xL7veOqDudd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# 假设我们已经有了一个向量存储\n",
        "vectorstore = FAISS.from_texts([\"Text about AI\", \"Text about ML\"], embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Context: {context}\\nQuestion: {question}\")\n",
        "#model = ChatOpenAI()\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "\n",
        "result = chain.invoke(\"What is AI?\")\n",
        "print(result)\n",
        "\n",
        "#AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems. This can include tasks such as learning, reasoning, problem-solving, perception, and language understanding. AI technologies are designed to mimic human cognitive functions and are used in a wide range of applications, from speech recognition and image processing to autonomous vehicles and medical diagnosis.\n",
        "# AI, or Artificial Intelligence, refers to the simulation of human intelligence in machines that are designed to think and act like humans. This can include a variety of capabilities such as learning from experience, understanding natural language, recognizing patterns, and making decisions. AI can be categorized into different types, including narrow AI, which is designed for specific tasks, and general AI, which has the potential to understand and perform any intellectual task that a human can do. Key technologies in AI include machine learning, natural language processing, and robotics."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrIu1OwfuWMi",
        "outputId": "3911dfd6-ffea-48b2-ce9b-581c7d59783c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI, or Artificial Intelligence, refers to the simulation of human intelligence in machines that are designed to think and act like humans. AI systems can perform tasks that typically require human intelligence, such as understanding natural language, recognizing patterns, solving problems, and making decisions. These systems are built on algorithms and data, allowing them to learn from experience and improve their performance over time. AI encompasses various subfields, including machine learning, natural language processing, robotics, and computer vision, and is applied in numerous applications such as virtual assistants, autonomous vehicles, and predictive analytics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePyc7Qheru7a",
        "outputId": "0ef1a50e-9b0a-40c6-a5ab-e946525fdf14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Context: {context}\\nQuestion: {question}'))])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3, RunnablePassthrough()"
      ],
      "metadata": {
        "id": "v9qCZUCNu7Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Tell me about {topic}\")\n",
        "model = OpenAI()\n",
        "\n",
        "chain = RunnablePassthrough() | prompt | model\n",
        "\n",
        "result = chain.invoke(\"AI\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfKodQfJurzg",
        "outputId": "3567f08b-de4d-41f8-acf6-195385c4b93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "AI, or artificial intelligence, is the branch of computer science that focuses on creating intelligent machines that can think, learn, and solve problems in a way that mimics human cognition. AI involves the development of algorithms and computer programs that enable machines to process and analyze large amounts of data, recognize patterns, make decisions, and improve their performance over time. Some common applications of AI include speech recognition, image recognition, natural language processing, and robotics. AI has the potential to revolutionize many industries, from healthcare and finance to transportation and manufacturing, by automating tasks and improving efficiency. However, there are also concerns about the ethical implications of creating machines that can think and act like humans, as well as the potential for AI to surpass human intelligence in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7APIy5Fyw2cx",
        "outputId": "806ffa09-66d1-4dcf-cc2e-b8ff6cc4e7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['topic'], template='Tell me about {topic}')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4, RunnableParallel()"
      ],
      "metadata": {
        "id": "5cgT5LFyvT9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableParallel\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 初始化 OpenAI 模型\n",
        "model = OpenAI()\n",
        "\n",
        "# 创建提示模板\n",
        "summarize_prompt = PromptTemplate.from_template(\"Summarize the following: {text}\")\n",
        "translate_prompt = PromptTemplate.from_template(\"Translate the following to French: {text}\")\n",
        "\n",
        "# 创建可运行的链\n",
        "summarize_chain = summarize_prompt | model\n",
        "translate_chain = translate_prompt | model\n",
        "\n",
        "# 创建并行运行的链\n",
        "parallel_chain = RunnableParallel(\n",
        "    summary=summarize_chain,\n",
        "    translation=translate_chain\n",
        ")\n",
        "\n",
        "# 调用并行链\n",
        "result = parallel_chain.invoke({\"text\": \"Hello, how are you?\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU7ZyEh7vVH-",
        "outputId": "03824f7b-be17-40ce-b941-b32c01bb9c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'summary': \"\\n\\nThe statement is a greeting and a question about someone's well-being.\", 'translation': '\\nBonjour, comment ça va ?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5, RunnableSequence()"
      ],
      "metadata": {
        "id": "1458nK7bvyjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "model = OpenAI(model_name='gpt-4o-mini-2024-07-18')  #默认使用的是 gpt-3.5-turbo-instruct\n",
        "\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "chain.invoke({\"topic\": \"bears\"})\n",
        "\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
        "\n",
        "composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()\n",
        "composed_chain.invoke({\"topic\": \"bears\"})\n",
        "\n",
        "composed_chain_with_lambda = (\n",
        "    chain\n",
        "    | (lambda input: {\"joke\": input})\n",
        "    | analysis_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "composed_chain_with_lambda.invoke({\"topic\": \"bears\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tBsyhraLy1d0",
        "outputId": "ce87e689-2729-444d-cac2-099c105c763d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's a cute joke! It's lighthearted and unexpected, which can definitely make it funny. Humor often comes from surprise or absurdity, and the image of a bear in a sweater is pretty amusing. Would you like to hear another joke?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def add_one(x: int) -> int:\n",
        "    return x + 1\n",
        "\n",
        "def mul_two(x: int) -> int:\n",
        "    return x * 2\n",
        "\n",
        "runnable_1 = RunnableLambda(add_one)\n",
        "runnable_2 = RunnableLambda(mul_two)\n",
        "#sequence = runnable_1 | runnable_2\n",
        "# Or equivalently:\n",
        "sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
        "sequence.invoke(1)\n",
        "await sequence.ainvoke(1)\n",
        "\n",
        "sequence.batch([1, 2, 3])\n",
        "await sequence.abatch([1, 2, 3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0Ri0WBD0xp3",
        "outputId": "81aab4cd-400e-4aa2-8916-2301aa4e89ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 6, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "model = OpenAI(model_name='gpt-4o-mini-2024-07-18')  #默认使用的是 gpt-3.5-turbo-instruct\n",
        "\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "chain.invoke({\"topic\": \"bears\"})\n",
        "\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
        "\n",
        "composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()\n",
        "composed_chain.invoke({\"topic\": \"bears\"})\n",
        "\n",
        "composed_chain_with_lambda = (\n",
        "    chain\n",
        "    | (lambda input: {\"joke\": input})\n",
        "    | analysis_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "composed_chain_with_lambda.invoke({\"topic\": \"bears\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fGG-CnN5vWnI",
        "outputId": "616d7fe0-d40b-4b30-9832-1025e968e3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's a cute and light-hearted joke! It has a playful twist, imagining bears in sweaters and the silliness of it. It's definitely suitable for a good chuckle, especially for kids. Do you have any more jokes like that?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6，RunnableLambda()"
      ],
      "metadata": {
        "id": "jjgwOeVPwOLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "def uppercase(text):\n",
        "    return text.upper()\n",
        "\n",
        "chain = RunnablePassthrough() | RunnableLambda(uppercase)\n",
        "\n",
        "result = chain.invoke(\"hello world\")\n",
        "print(result)  # 输出: HELLO WORLD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk9xNofQwTp0",
        "outputId": "52feceba-500a-48d8-d2b1-8bb1827d5529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO WORLD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsdXIPaIwULh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7, RunnableMap()"
      ],
      "metadata": {
        "id": "6HfQGZgtwVs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableMap\n",
        "\n",
        "text_processor = RunnableMap({\n",
        "    \"uppercase\": lambda x: x.upper(),\n",
        "    \"lowercase\": lambda x: x.lower(),\n",
        "    \"length\": lambda x: len(x)\n",
        "})\n",
        "\n",
        "result = text_processor.invoke(\"Hello World\")\n",
        "print(result)  # 输出: {'uppercase': 'HELLO WORLD', 'lowercase': 'hello world', 'length': 11}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuepJcNPwYWT",
        "outputId": "15ffd38e-29a4-49bb-d6de-aae70c17cd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uppercase': 'HELLO WORLD', 'lowercase': 'hello world', 'length': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XytAxJvZwZxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8, RunnableBranch()"
      ],
      "metadata": {
        "id": "fx21J2ZPwalY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 初始化 OpenAI 模型\n",
        "model = OpenAI()\n",
        "\n",
        "# 创建提示模板\n",
        "short_prompt = PromptTemplate.from_template(\"Give a short answer: {input}\")\n",
        "long_prompt = PromptTemplate.from_template(\"Give a detailed explanation: {input}\")\n",
        "\n",
        "# 创建短答案和长答案链\n",
        "short_chain = short_prompt | model\n",
        "long_chain = long_prompt | model\n",
        "\n",
        "# 创建分支链\n",
        "chain = RunnableBranch(\n",
        "    (lambda x: len(x) > 20, long_chain),\n",
        "    short_chain\n",
        ")\n",
        "\n",
        "# 调用链\n",
        "result_short = chain.invoke(\"What is AI?\")\n",
        "result_long = chain.invoke(\"Explain the differences between AI, ML, and deep learning\")\n",
        "\n",
        "print(result_short)\n",
        "print(result_long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYV6i5H8wexE",
        "outputId": "16ca4f8b-e769-4424-beb4-b3595c535dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "AI stands for artificial intelligence. It is a branch of computer science that focuses on creating intelligent machines that can think and learn like humans, and perform tasks that typically require human intelligence.\n",
            "\n",
            "\n",
            "AI (Artificial Intelligence) is a broad field that aims to create intelligent machines that can think and act like humans. It involves developing algorithms and computer programs that can perform tasks that typically require human intelligence, such as problem-solving, decision making, and understanding complex data.\n",
            "\n",
            "ML (Machine Learning) is a subset of AI that involves training machines to learn from data and improve their performance without being explicitly programmed. It uses statistical techniques to enable machines to identify patterns and make predictions or decisions based on the data they have been given.\n",
            "\n",
            "Deep Learning is a subfield of ML that uses artificial neural networks to mimic the structure and function of the human brain. It involves feeding large amounts of data into these neural networks, which then learn to recognize patterns and make predictions based on that data. Deep learning is often used for tasks such as image and speech recognition, natural language processing, and autonomous driving.\n",
            "\n",
            "One key difference between AI, ML, and deep learning is the level of human involvement required. AI and ML algorithms often require human input to define the problem, choose the data, and design the features to be extracted from the data. In contrast, deep learning algorithms can automatically learn and extract features from the data, reducing the need for human intervention.\n",
            "\n",
            "Another difference is the type of problems each\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9, 条件执行"
      ],
      "metadata": {
        "id": "QEXub03Gwuwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "model = OpenAI()\n",
        "formal_prompt = PromptTemplate.from_template(\"Respond formally: {input}\")\n",
        "casual_prompt = PromptTemplate.from_template(\"Respond casually: {input}\")\n",
        "\n",
        "def is_formal(text):\n",
        "    return \"sir\" in text.lower() or \"madam\" in text.lower()\n",
        "\n",
        "def chain(x):\n",
        "    prompt = formal_prompt if is_formal(x) else casual_prompt\n",
        "    return model(prompt.format(input=x))\n",
        "\n",
        "result_formal = chain(\"Hello sir, how are you?\")\n",
        "result_casual = chain(\"Hey, what's up?\")\n",
        "print(result_formal)\n",
        "print(result_casual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdI0iGcAw1LQ",
        "outputId": "47fd9c60-8e0f-41fc-867f-407585c69f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Hello, I am an AI and do not have the ability to feel emotions. However, thank you for asking. How may I assist you?\n",
            " How's your day going?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10, 异步支持"
      ],
      "metadata": {
        "id": "hURn7R1mxC9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "model = OpenAI()\n",
        "prompt = PromptTemplate.from_template(\"Tell me a fact about {topic}\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "async def run_async():\n",
        "    result = await chain.ainvoke({\"topic\": \"space\"})\n",
        "    print(result)\n",
        "\n",
        "# Call the async function directly (no need for asyncio.run)\n",
        "await run_async()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxR7ZNbdw_ao",
        "outputId": "a47d0597-4d8e-470b-bdfd-bec5dcfd3d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Space is completely silent because sound waves need a medium to travel through, and there is no medium in the vacuum of space. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11，流式处理\n"
      ],
      "metadata": {
        "id": "HqWpny_-xLm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "model = OpenAI()\n",
        "prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "for chunk in chain.stream({\"topic\": \"a brave knight\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXNC-0EjxJLv",
        "outputId": "54ab825d-bcdc-46b0-eafb-c279406ede80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Once upon a time in a land far away, there lived a brave knight named Sir William. He was known throughout the kingdom for his courage, loyalty, and strength. Sir William had been a knight since he was a young boy, following in the footsteps of his father who was also a great knight.\n",
            "\n",
            "One day, the kingdom was suddenly attacked by an evil dragon. The dragon had been terrorizing the kingdom for months, burning villages and causing chaos wherever it went. The king called upon Sir William to defeat the dragon and save the kingdom.\n",
            "\n",
            "Without hesitation, Sir William mounted his trusty steed and set off to confront the dragon. As he rode through the kingdom, he saw the destruction the dragon had caused. He knew he had to act fast before more innocent lives were lost.\n",
            "\n",
            "When Sir William finally reached the dragon's lair, he was met with a fierce roar. The dragon was enormous, with scales as black as night and eyes that glowed red with anger. But Sir William did not falter. He drew his sword and charged towards the dragon.\n",
            "\n",
            "The battle was long and grueling. The dragon breathed fire and swiped its sharp claws at Sir William, but he was quick and agile, dodging each attack. As they fought, the dragon"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7b_H73wuxQdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12，批处理"
      ],
      "metadata": {
        "id": "ZzRVvTUMxSx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "model = OpenAI()\n",
        "prompt = PromptTemplate.from_template(\"Translate '{word}' to French\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "words = [\"hello\", \"world\", \"AI\"]\n",
        "results = chain.batch([{\"word\": word} for word in words])\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t4LVD7vxUT1",
        "outputId": "da104d19-fe62-4d98-90bb-8003af1ad830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "bonjour\n",
            "\n",
            "\n",
            "monde\n",
            "\n",
            "\n",
            "AI translates to \"IA\" in French.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13，配置和自定义"
      ],
      "metadata": {
        "id": "aCvvDvg84ijC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0).configurable_fields(\n",
        "    temperature=ConfigurableField(\n",
        "        id=\"llm_temperature\",\n",
        "        name=\"LLM Temperature\",\n",
        "        description=\"The temperature of the LLM\",\n",
        "    )\n",
        ")\n",
        "\n",
        "model.invoke(\"pick a random number\")\n",
        "\n",
        "\n",
        "model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")\n",
        "\n",
        "#AIMessage(content='34')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "01-60alp4lmH",
        "outputId": "26c59b14-73b8-41b3-8693-c22783f00a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'OpenAI'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-462224deb6a1>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = ChatOpenAI(temperature=0).configurable_fields(\n\u001b[0m\u001b[1;32m      6\u001b[0m     temperature=ConfigurableField(\n\u001b[1;32m      7\u001b[0m         \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llm_temperature\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrorWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROOT_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"openai_proxy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root_client\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"client\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root_client\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"async_client\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-openai ope"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_M0yQ9L5SIy",
        "outputId": "64d0a1b8-726f-4ac5-e800-5aee28ef8bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m143.4/144.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for screed (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --quiet  langchain langchain-openai\n",
        "\n",
        "from langchain_community.chat_models import ChatAnthropic\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from unittest.mock import patch\n",
        "\n",
        "import httpx\n",
        "from openai import RateLimitError\n",
        "\n",
        "request = httpx.Request(\"GET\", \"/\")\n",
        "response = httpx.Response(200, request=request)\n",
        "error = RateLimitError(\"rate limit\", response=response, body=\"\")\n",
        "\n",
        "# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc\n",
        "openai_llm = ChatOpenAI(max_retries=0)\n",
        "anthropic_llm = ChatAnthropic()\n",
        "llm = openai_llm.with_fallbacks([anthropic_llm])\n",
        "\n",
        "# Let's use just the OpenAI LLm first, to show that we run into an error\n",
        "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
        "    try:\n",
        "        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n",
        "    except RateLimitError:\n",
        "        print(\"Hit error\")\n",
        "\n",
        "\n",
        "#Hit error"
      ],
      "metadata": {
        "id": "2DZrM7AX6AKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "LbRUhY2ixaHV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
